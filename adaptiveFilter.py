class regrad(object):
	# при задании класса указываем размер окна (n)
	# и коэффициент подстройки
	def __init__(self, n, k):
		# размерность входного вектора предыдущих значений
		self.n = n
		# начальный размер весовых коэффициентов
		w0 = 1.0/n
		# весовые коэффициенты
		self.w = n * [w0]
		# подстроечный коэффициент
		self.k = k

	# подаем на вход массив из n элементов
	# на выходе прогноз (n+1)-го элемента
	def progn(self, x):
		s = 0.
		for i in range(len(x)):
			s += self.w[i] * x[i]
		return s

	# метод настройки - на вход массив n элементов
	# и реальное значение (n+1)-го элемента
	# на выходе - величина ошибки
	def tune(self, x, xr):
		xp = self.progn(x)
		e = (xr - xp)**2
		for i in range(self.n):
			self.w[i] += 2*self.k*e*x[i]
		return e ** 0.5


def prohod(x, t, n = 10):
	#n -- задаем разменр окна
	# указываем любое большое значение
	# это переменная для нахождения минимальной средней погрешности 
	emin = 20000
	mas = []
	# проводим тысячу экпериментов...
	for k in range(1000):
		# ... с различными коэффициентами подстройки
		a = float(k)/t
		# создаем фильтр
		r = regrad(n, a)
		# переменная для суммирования
		s = 0.
		# делаем 50 прогнозов
		for i in range(50):
			# суммируем ошибки
			s += r.tune(x[i:i+n], x[i+n])

		# находим среднюю ошибку 1-го прогноза
		s = s/50
		# если ошибка минимальна ...
		if s < emin:
			# ... запоминаем ее
			an = r
			emin = s
		# удаляем объект класса
		del(r)

	# выводим минимальную погрешность
	print('minimum', emin)
	return an